# built-in
import os
import argparse
from datetime import datetime, timezone, timedelta
from dateutil import parser as date_parser
from typing import List, Dict, Any, Optional, Set

# python-dotenv
from dotenv import load_dotenv

# ingestion
from ingestion import confluence_client
from ingestion import preprocessing
from ingestion import storage

# utils
from utils import times

load_dotenv()

SPACE_KEY = os.getenv("SPACE_KEY")
MAX_PAGES_PER_REQUEST = 100  # Confluence APIì˜ ê¸°ë³¸ ì œí•œ



def get_all_pages_in_space(confluence, space_key: str) -> List[Dict[str, Any]]:
    """
    Confluence ê³µê°„ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜í•˜ì—¬ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        confluence: Confluence í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        space_key: í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ê³µê°„ì˜ í‚¤
    
    Returns:
        ëª¨ë“  í˜ì´ì§€ ëª©ë¡
    """
    all_pages = []
    start = 0
    limit = MAX_PAGES_PER_REQUEST

    while True:
        pages = confluence.get_all_pages_from_space(
            space_key, 
            start=start, 
            limit=limit, 
            expand='version'
        )
        
        if not pages:
            break
        
        all_pages.extend(pages)
        
        # í˜ì´ì§€ ìˆ˜ê°€ ì œí•œë³´ë‹¤ ì ìœ¼ë©´ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ëŠ” ê²ƒ
        if len(pages) < limit:
            break
        
        start += limit

    return all_pages


def ingest_all_pages(
    confluence, 
    collection, 
    space_key: Optional[str] = None, 
    page_ids: Optional[List[str]] = None, 
    exclude_ids: Optional[Set[str]] = None, 
    limit: int = 5000, 
    after_date: Optional[datetime] = None
):
    """
    í˜ì´ì§€ë¥¼ ì¸ì œìŠ¤íŠ¸í•˜ê³  í•„ìš”í•œ ê²½ìš° ë²¡í„° DBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    exclude_ids = set(exclude_ids or [])
    
    # í˜ì´ì§€ ëª©ë¡ ê²°ì •
    if page_ids:
        pages = [confluence.get_page_by_id(pid, expand='version,body.view') for pid in page_ids]
    else:
        space_key = space_key or SPACE_KEY
        pages = get_all_pages_in_space(confluence, space_key)
    
    total_pages = len(pages)
    processed_pages = skipped_pages = error_pages = 0

    print(f"ğŸ“‹ START: ì´ {total_pages}ê°œ í˜ì´ì§€ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    # after_dateë¥¼ timezone-awareë¡œ ë³€í™˜
    if after_date:
        after_date = times.ensure_timezone_aware(after_date)

    for page in pages:
        if processed_pages >= limit:
            print(f"â¹ï¸ LIMIT: ì§€ì •ëœ í˜ì´ì§€ í•œê³„({limit})ì— ë„ë‹¬í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        page_id = page["id"]

        if page_id in exclude_ids:
            print(f"ğŸš« SKIP: í˜ì´ì§€ ID {page_id} (ì œëª©: {page['title']})ëŠ” ì œì™¸ ëª©ë¡ì— ìˆì–´ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
            skipped_pages += 1
            continue

        try:
            # API í˜¸ì¶œ ì‹œ body.viewë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™•ì¥
            page_detail = confluence.get_page_by_id(page_id, expand="version,body.view")
            
            # í˜ì´ì§€ ë””í…Œì¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            print(f"ğŸ“ í˜ì´ì§€ ìƒì„¸: {page_detail.keys()}")

            # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì¶œ
            last_updated = times.ensure_timezone_aware(date_parser.isoparse(page_detail["version"]["when"]))

            if after_date and last_updated < after_date:
                print(f"ğŸ“… SKIP: í˜ì´ì§€ ID {page_id} (ì œëª©: {page['title']})ëŠ” ì§€ì •í•œ ë‚ ì§œ({after_date.date()}) ì´ì „ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                skipped_pages += 1
                continue

            page_title = page["title"]
            print(f"âœ… PROCESS: {page_title} (ID: {page_id})")

            # body í‚¤ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            html_content = page_detail.get("body", {}).get("view", {}).get("value", "")
            
            if not html_content:
                print(f"âš ï¸ SKIP: í˜ì´ì§€ ID {page_id} (ì œëª©: {page_title})ì˜ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                skipped_pages += 1
                continue

            text_content = preprocessing.html_to_text(html_content)

            if not text_content:
                print(f"âš ï¸ SKIP: í˜ì´ì§€ ID {page_id} (ì œëª©: {page_title})ì˜ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                skipped_pages += 1
                continue

            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            existing_ids = collection.get(where={"page_id": page_id}).get("ids", [])
            if existing_ids:
                collection.delete(ids=existing_ids)

            # ìƒˆ ì²­í¬ ì¶”ê°€
            chunks = preprocessing.chunk_text(text_content)
            new_ids = [f"{page_id}-{i}" for i in range(len(chunks))]
            metadatas = [{"page_id": page_id, "title": page_title}] * len(chunks)
            collection.add(ids=new_ids, documents=chunks, metadatas=metadatas)

            processed_pages += 1

        except KeyError as e:
            print(f"âŒ KEY ERROR: í˜ì´ì§€ ID {page_id} (ì œëª©: {page['title']}) ì²˜ë¦¬ ì¤‘ í‚¤ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"í˜ì´ì§€ ìƒì„¸ ì •ë³´: {page}")
            error_pages += 1
        except Exception as e:
            print(f"âŒ ERROR: í˜ì´ì§€ ID {page_id} (ì œëª©: {page['title']}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"í˜ì´ì§€ ìƒì„¸ ì •ë³´: {page}")
            error_pages += 1

    print("\nğŸ“Š SUMMARY:")
    print(f"ğŸ”¹ ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
    print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ í˜ì´ì§€: {processed_pages}")
    print(f"â© ê±´ë„ˆë›´ í˜ì´ì§€: {skipped_pages}")
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ í˜ì´ì§€: {error_pages}")


def main():
    parser = argparse.ArgumentParser(description="Confluence ë°ì´í„°ë¥¼ ë²¡í„°DBì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸")

    parser.add_argument("--all", action="store_true", help="ëª¨ë“  í˜ì´ì§€ë¥¼ ì²˜ë¦¬")
    parser.add_argument("--ids", nargs="+", help="ì²˜ë¦¬í•  íŠ¹ì • í˜ì´ì§€ ID ëª©ë¡")
    parser.add_argument("--exclude", nargs="+", help="ì œì™¸í•  í˜ì´ì§€ ID ëª©ë¡ (ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì‹œ ì‚¬ìš©)")
    parser.add_argument("--limit", type=int, default=5000, help="ì²˜ë¦¬í•  í˜ì´ì§€ ìµœëŒ€ ê°œìˆ˜")
    parser.add_argument("--after-date", type=str, help="YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ, ì§€ì • ë‚ ì§œ ì´í›„ë¡œ ìƒì„± ë˜ëŠ” ìˆ˜ì •ëœ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
    parser.add_argument("--space", type=str, default=SPACE_KEY, help="ì²˜ë¦¬í•  Confluence ê³µê°„ í‚¤")
    parser.add_argument("--recent", action="store_true", help="ìµœê·¼ í•˜ë£¨ ì´ë‚´ì— ìƒì„± ë˜ëŠ” ìˆ˜ì •ëœ í˜ì´ì§€ë§Œ ì²˜ë¦¬")

    args = parser.parse_args()

    if not args.all and not args.ids:
        parser.error("âš ï¸ ìµœì†Œí•œ í•˜ë‚˜ì˜ ì˜µì…˜(--all ë˜ëŠ” --ids)ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # Check for conflicting options
    if args.recent and args.after_date:
        print("âš ï¸ WARNING: --recentì™€ --after-date ì˜µì…˜ì´ ë™ì‹œì— ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. --recent ì˜µì…˜ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")
        args.recent = False

    confluence = confluence_client.create_confluence_client()
    collection = storage.init_chromadb()

    exclude_ids = set(args.exclude or [])

    after_date = None
    if args.after_date:
        after_date = datetime.strptime(args.after_date, "%Y-%m-%d")
    elif args.recent:
        # í˜„ì¬ ì‹œê°„ì—ì„œ 1ì¼ ì „ ë‚ ì§œ ê³„ì‚°
        after_date = datetime.now(timezone.utc) - timedelta(days=1)

    if args.all:
        ingest_all_pages(
            confluence, 
            collection, 
            space_key=args.space, 
            exclude_ids=exclude_ids, 
            limit=args.limit, 
            after_date=after_date
        )

    if args.ids:
        ingest_all_pages(
            confluence, 
            collection, 
            page_ids=args.ids, 
            after_date=after_date
        )

    print("\nğŸ‰ ì‘ì—… ì™„ë£Œ")


if __name__ == "__main__":
    main()